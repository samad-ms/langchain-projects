{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x2007ec17700>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##1) data ingestion\n",
    "# 1)text loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(\"speech.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n**Title: The Role of Machine Learning in Modern Cyberbullying Detection**\\n\\n*Introduction:*\\n\\n\"Good [morning/afternoon/evening] everyone,\\n\\nMy name is Abdul Samad, a passionate Data Scientist based in Kerala, India. Today, I am honored to present my work on \\'The Role of Machine Learning in Modern Cyberbullying Detection.\\' As someone deeply invested in the potential of artificial intelligence, I have dedicated a significant portion of my career to developing solutions that can make our online spaces safer and more respectful.\\n\\n*Background:*\\n\\nCyberbullying has become a pervasive issue in the digital age. With the rise of social media and online communication platforms, the need for effective detection and prevention strategies is more critical than ever. Traditional methods of addressing cyberbullying often fall short due to the sheer volume of content and the nuanced nature of online interactions.\\n\\n*Our Solution - CyberGuard:*\\n\\nIn response to this challenge, I developed a project called CyberGuard. CyberGuard is an AI-powered platform designed to detect and transform cyberbullying into a more polite and respectful form without losing the original context. This solution ensures that negative comments remain negative but are stripped of their harmful impact.\\n\\n*Key Features and Techniques:*\\n\\nCyberGuard leverages advanced machine learning techniques, including Natural Language Processing (NLP) and Generative AI. By utilizing pre-trained models from the Hugging Face library and fine-tuning them with the Lora PEFT technique, we achieved a high level of accuracy and efficiency in identifying and transforming harmful language.\\n\\nOne of the main challenges we faced was maintaining the context of the original message while removing its harmful elements. Through rigorous testing and iterative improvements, we developed a system that effectively balances these requirements. Our efforts were rewarded when CyberGuard was recognized for its innovation and practical impact in the field of AI-driven social solutions.\\n\\n*Future Directions:*\\n\\nLooking ahead, we aim to extend CyberGuard\\'s capabilities to various online platforms, ensuring its adaptability and scalability. Additionally, we are exploring the integration of distributed training using Azure Cloud servers to enhance the model\\'s performance further. Our ultimate goal is to create an online environment where constructive criticism can flourish without the adverse effects of cyberbullying.\\n\\n*Conclusion:*\\n\\nIn conclusion, the application of machine learning in cyberbullying detection and prevention represents a significant step forward in creating safer online communities. I am incredibly proud of the work we have accomplished with CyberGuard and excited about its future potential. I believe that through continued innovation and collaboration, we can make the internet a more respectful and inclusive space for everyone.\\n\\nThank you for your attention. I am happy to take any questions you may have.\"\\n', metadata={'source': 'speech.txt'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x2006e9daef0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1)WebBaseLoader\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader=WebBaseLoader(web_path=('https://lilianweng.github.io/posts/2023-06-23-agent/'),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "\n",
    "                     )))\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x2007ec14af0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3)PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(file_path='ml.pdf')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Machine Learning  \\n \\nMachine learning is a subset of artificial intelligence (AI) that focuses on \\ndeveloping computer systems capable of learning and improving from data \\nwithout being explicitly programmed.  Machine learning is teaching computers to \\nlearn from data and make decisions or predictions based on that learning.  \\nMachine learning is widely used in various fields and applications  such as:  \\n• Face Recognition  \\n• Object Detection  \\n• Chatbots  \\n• Recommendation Systems  \\n• Autonomous Vehicles  \\n• Disease Diagnosis  \\n• Fraud detection  \\nAnd many more it is nowadays being used in almost all sectors including \\nhealthcare, education, business, construction, astronomy, etc.  \\n \\nTypes of Machine Learning : \\n \\nThere are many types of machine learning but the most famous types are:  \\n➢ Supervised learning  \\n➢ Unsupervised learning  \\n➢ Reinforcement learning  \\n \\n \\n \\n ', metadata={'source': 'ml.pdf', 'page': 0}),\n",
       " Document(page_content=\"Supervised Learning  \\n \\nIn supervised learning the algorithm is trained on labeled data which means \\ntraining data includes the input and output pairs. The goal is to learn the mapping \\nfrom input to output.  \\nX -> Y \\nSome examples of supervised learning include image classification, email spam \\ndetection, and predicting software.  \\n \\nTypes of supervised learning tasks:  \\n \\n1. Regression  \\n2. Classification  \\n \\nRegression:  \\nRegression is a type of supervised learning task where the algorithm's goal is to \\npredict a continuous numerical output or target variable.  In regression, the output \\nis a real -valued number, and the algorithm's objective is to learn a mapping from \\ninput features to this continuous output.  \\nExamples of regression tasks include home price prediction, black hole mass \\nprediction, stock price prediction, age estimation, etc.  \\nAlgorithms:  \\n❖ Linear Regression:  \\n \\nLinear Regression is a fundamental statistical and machine -learning  \\ntechnique  for solving regression problems  used for modeling the \\nrelationship between a dependent variable (or target) and one or more \", metadata={'source': 'ml.pdf', 'page': 1}),\n",
       " Document(page_content='independent variables (or features). It assumes that this relationship is \\napproximately linear, meaning that changes in the independent variables \\nhave a linear effect on the dependent variable.  \\n➢ Simple Linear Regression:  \\nIn simple linear regression, there is only one input feature.  \\nY= wx+b  \\nY= target variable  \\nW=slope of straight line  \\n X=input features  \\n b= y-intercept  \\n➢ Multiple Linear Regression:  \\nIn multiple linear regression, there is more than one input feature.  \\nY=w1x1+w2x2+w3x3+…. +b  \\n \\n \\n \\n  \\n \\n', metadata={'source': 'ml.pdf', 'page': 2}),\n",
       " Document(page_content=' Note: There are 2 common issues in machine learning and statistical \\nmodeling that are underfitting  and overfitting . \\nUnderfitting:  \\nUnderfitting occurs when a model is too simple to capture the underlying patterns \\nin the data, both in the training set and unseen data.  It essentially means the \\nmodel is not complex enough to represent the relationships between the input \\nfeatures and the target variable.  This means it is not predicting the output \\ncorrectly and the accuracy of the model is very low.  Solutions to overcome this \\nproblem are:  \\n1) Increase model complexity.  \\n2) Add more features.  \\n3) Feature engineering (It is basically creating the new input feature using the \\nexisting input features For example for home price prediction you are given \\nthe width and length of a home you can simply create another feature \\nnamed area of home by multiplying  the width and length of home you \\nalready know).  \\n4) Remove noise (clean data to remove outliers).  \\n5) Increase training data.  \\n6) Increase training data.  \\n7) Reduce regularization.  \\n8) Iterating the optimization algorithms like gradient descent for more times.  \\nOverfitting:  \\nOverfitting occurs when a model is excessively complex and fits the training data \\nnoise rather than the underlying patterns.  It means the model is too flexible and \\nessentially memorizes the training data rather than generalizing it.  Simply we can \\nsay that in this case the cost function  (The cost function quantifies the error \\nbetween predicted and expected values and presents that error in the form of a \\nsingle real number.)   is very low close to 0 and the accuracy of the model is \\naround 100%.  Solutions to overcome this problem are:  \\n1) Collect more training data  \\n2) Feature selection  \\n3) Feature engineering  ', metadata={'source': 'ml.pdf', 'page': 3}),\n",
       " Document(page_content=\"4) Do regularization.  \\nRegularization:  \\nRegularization is a technique used in machine learning and statistics to prevent \\noverfitting, which occurs when a model fits the training data too closely, \\ncapturing noise and making it less effective at making predictions on new, \\nunseen data. Regularizati on adds a penalty term to the model's loss function, \\ndiscouraging it from fitting the training data too precisely and encouraging it to \\nfind a simpler, more generalizable solution.  There are 2 of its common \\ntechniques.  \\nA. L1 regularization (lasso):  \\nL1 regularization adds the absolute values of the model's parameters to the \\nloss function.  Lasso is useful for feature selection and simplifying models.  \\n                \\nB. L2 regularization (ridge):  \\nL2 regularization adds the squared values of the model's parameters to the \\nloss function . Ridge helps reduce model complexity and is effective when all \\nfeatures are potentially relevant . \\n \\n \\n  \\nNote :  Learning rate  is a hyperparameter in machine learning algorithms, \\nparticularly in optimization algorithms like gradient descent. It determines the size \\nof steps that the algorithm takes when adjusting the model's parameters during \\n\", metadata={'source': 'ml.pdf', 'page': 4}),\n",
       " Document(page_content=\"training . Choosing a small learning rate can lead you to converge to the solution \\nslowly and by choosing a large value of learning rate the solution does not \\nconverge to a point.  \\n \\nValidation set in Machine learning:  \\nA validation set is a portion of the dataset used in machine learning to assess the \\nperformance of a trained model. It's like a practice exam for the model before it \\nfaces the real test (the test set).  The validation set is not part of the training data, \\nnor is it part of the final evaluation data (the test set). Instead, it serves as a \\nmiddle ground for testing the model's performance during training.  It helps you \\nmake decisions about how well your model is learning and whether it's overfitting \\nor underfitti ng. During the training process, after each training epoch (iteration), \\nthe model's performance is evaluated on the validation set.  The model's \\npredictions on the validation set are compared to the actual target values, and a \\nperformance metric (like accuracy, loss, or other relevant metrics) is computed.  \\nThis metric helps you understand how well the model is doing on data it hasn't \\nseen during training.  You can adjust hyperparameters (like learning rate, model \\narchitecture, or regularization strength) based on the model's performance on the \\nvalidation set. For example, if the model's perf ormance on the validation set starts \\nto degrade, you may want to reduce the learning rate or simplify the model.  \\nFeature Scaling in Machine Learning:  \\nFeature scaling is a preprocessing technique in machine learning that helps bring \\nall the features (variables) of your dataset onto a similar scale. It ensures that no \\nsingle feature dominates the learning process because of its larger magnitude.  \\nDifferent features in your dataset may have values on different scales. For \\nexample, one feature might range from 0 to 1, while another might range from 0 \\nto 1000.  Feature scaling is used to make sure that these varying scales do not \\naffect the performance of machine learning algorithms.  Feature scaling involves \\ntransforming the values of each feature so that th ey fall within a similar numerical \\nrange.  The common techniques used to do feature scaling are min -max scaling \\nand z -score. Feature scaling ensures that all features contribute equally to the \\nlearning process, preventing some features from having undue influence due to \", metadata={'source': 'ml.pdf', 'page': 5}),\n",
       " Document(page_content=\"their larger values.  It can lead to faster convergence of gradient -based \\noptimization algorithms (like gradient descent).  \\n❖ Polynomial Regression:  \\n \\nPolynomial regression is a type of regression analysis used in machine \\nlearning and statistics to model relationships between variables when the \\ntraditional linear regression model is insufficient. It extends the concept of \\nlinear regression by allowing fo r more complex, nonlinear relationships \\nbetween the independent and dependent variables.  \\ny = b₀ + b₁x + b₂x² + ... + b ₙxⁿ \\n \\n \\n \\nThe degree of the polynomial (n) is a hyperparameter that you can choose \\nbased on the complexity of the relationship you want to capture.  \\nA higher degree allows the model to fit the data more closely but may also \\nlead to overfitting if not chosen carefully.  \\nIn order if overfitting occurs then reduce the degree of polynomial, collect \\nmore data do regularization.  \\n \\n❖ KNN regression:  \\n \\nK-Nearest Neighbors (KNN) regression is a simple and intuitive machine \\nlearning algorithm used for regression tasks. It's an instance -based learning \\nmethod that makes predictions by considering the average (or weighted \\n\", metadata={'source': 'ml.pdf', 'page': 6}),\n",
       " Document(page_content='average) of the target values of its k -nearest neighbors in the training data.  \\nKNN regression is used to predict a continuous target variable (numeric \\nvalue) based on the values of its neighboring data points.  During training, \\nthe KNN algorithm stores the entire training dataset.  \\n \\n \\nTo make a prediction for a new data point, the algorithm identifies the k -\\nnearest data points (neighbors) in the training set based on a distance \\nmetric (usually Euclidean distance).  K is a hyperparameter here. The \\ndistance is calculated between the entered point from all points in the \\nwhole data set then the data set is sorted based on distance in ascending \\norder and picked the first k rows and the mean of target variables are \\ncalcul ated which is the answer. The value of k is a hyperparameter that you \\nneed  to specify when using KNN regression.  A smaller k (e.g., 1 or 3) makes \\nthe model sensitive to noise in the data and can result in a more variable \\nprediction.  A larger k (e.g., 10 or 20) provides a smoother prediction but \\nmight not capture local patterns as effectively.  KNN regression is simple to \\nunderstand and implement.  It can capture complex and nonlinear \\nrelationships between features and the target variable.  Choosing the \\nappropriate value of k is crucial and can be challenging.  KNN can be \\ncomputationa lly expensive when the dataset is large, as it requires \\ncalculating distances to all data points during prediction.  KNN regression \\nworks well for small to moderately -sized  datasets with a reasonable number \\nof features.  For large datasets, the computational cost of finding nearest \\nneighbors can become prohibitive.  If overfitting occurs in KNN regression \\n', metadata={'source': 'ml.pdf', 'page': 7}),\n",
       " Document(page_content=\"then do better feature selection, adjust the value of k, do feature selection \\netc. \\n \\n❖ Decision Tree Regressor/ Regression tree : \\n \\nA regression tree is a type of decision tree used in machine learning for \\nregression tasks. It's a tree -like structure that makes predictions about \\ncontinuous numeric values.  Consider a scenario in which you are trying to \\npredict the drug usage effectiveness prediction.  \\n \\n \\nNow we have to build a tree and how should be built. Like what value of \\ndrug dosage should be the root node here is what we should know about \\nthe square sum residual. In the diagram let's focus on small 2 values and \\ntake an average of it that will be 3. No w imagine the node has a drug \\ndosage of less than 3. Now for all data below 3 calculate the average value \\nof effectiveness of all and also calculate the average value of effectiveness \\non data set above 3. Calculate the Squared sum residual in case of savin g it. \\nNow focus on the next 2 data points calculate the average of and consider \\nthe root to be valued less than 5 and do the same that we did for case of \\n\", metadata={'source': 'ml.pdf', 'page': 8}),\n",
       " Document(page_content='drug dosage less than 3 after that take the next data sets and do same. Now \\ncheck for the residuals you stored.  \\n                        \\n \\nAs seen select the dosage value with less residual value that is 14.5. Now \\nroot node will have a drug dosage of less than 14.5. Now after that, if the \\ndata sets less than 14.5 are really less values then no need to further split it \\njust calculate their ave rage value and make it a leaf node. For the data set \\nabove 14.5  split it into other nodes using the same concept that we did \\nearlier selecting the threshold value calculating the residuals and selecting \\none with less residual. The final tree will be like,  \\n                 \\n \\nHere the scenario we discussed has only one input feature If there are \\nmultiple features calculate the residuals for each feature and then select \\none with less value of residual and make the tree.  They are e asy to \\n', metadata={'source': 'ml.pdf', 'page': 9}),\n",
       " Document(page_content='understand and able  due to their tree -like structure.  It can  model complex, \\nnonlinear relationships in data.  Outliers have minimal impact on model \\nperformance.  But it is Sensitive to small changes in the data.  If overfitting \\nhappens in the regression tree then reduce the depth of the tree and do \\npruning.  \\nPruning:  \\nPruning is a technique used in decision tree -based models, including \\nregression trees, to prevent overfitting and improve model generalization.  \\nPruning involves cutting back or removing some branches (subtrees) of a \\ndecision tree after it has been fully grown.  It aims to simplify the tree by \\nremoving branches that capture noise or fine -grained details in the training \\ndata.  Pruning techniques consider a cost -complexity trade -off: They \\nevaluate the cost (error) associated with keeping or removing each subtree \\nand select the option that minimizes this cost.  Pruned subtrees are replaced \\nwith a single leaf node, often representing the average or majorit y class (for \\nclassification) or the average value (for regression) of the training data in \\nthat subtree.  \\n \\n❖ Random Forest Regression:  \\n \\nRandom Forest Regression is a machine learning method for predicting \\ncontinuous values, like house prices or temperatures . Random Forest \\nRegression is like having a group of decision  trees  who collaborate to make \\npredictions.  When making a prediction, the Random Forest collects \\nopinions from all the decision trees.  It combines these opinions by \\naveraging their predictions to arrive at a final, more accurate prediction.  If \\noverfitting happens in random forest regression then reduce the number of \\ntrees, featu re selection, pruning, etc. Random Forest Regression is less \\nprone to overfitting compared to a single Decision Tree.  Random Forests \\noften provide higher predictive accuracy compared to a single Decision \\nTree.  Random Forests are more stable and less sensitive to small variations \\nin the training data . While on the other hand,  Random Forests are more \\ncomplex than individual Decision Trees . Random Forests typically require ', metadata={'source': 'ml.pdf', 'page': 10}),\n",
       " Document(page_content=\"more computational resources and training time than a single Decision \\nTree . \\n \\n❖ Support Vector Regressor:  \\n \\nSVR is a machine learning algorithm used for regression tasks. It's an extension \\nof Support Vector Machines (SVMs) that were originally designed for \\nclassification.  The data points outside the tube of E ( eta)  to the regression line. \\nThey have the most influence on the positioning of the regression line  these \\ndata points are called support vectors . So, in SVR we also draw a marginal line \\nand our goal is to maximize the marginal line so maximum data points lie inside \\nthe marginal lines.  \\n \\n \\n                     \\n \\n\", metadata={'source': 'ml.pdf', 'page': 11}),\n",
       " Document(page_content='                                             \\n \\n So the cost function will be  \\n                                               \\nAnd our aim is to minimize the cost function.  Where c controls how many data \\npoints can be allowed to exist outside the margin (the region around the \\nregression line).  so a smaller value of c means that there are fewer  values \\noutside the margin line and model  accuracy will be high and a high value of c \\nthat there will be more points outside the margin line and accuracy will be low . \\nWhile other parameter tells us about the distance between the data point \\noutside the marginal line and the marginal line. So he re is the scenario we \\nreduce the cost function in the way and make a model. So in SVR   kernel is \\nbasically a function that transforms the  data into higher dimensions  in order to \\nbuild  a model SVR  that can handle outliers effectively. Training an SVR model \\ncan be computationally expensive, especially on very large datasets.  In order to \\navoid overfitting in the Support vector regressor we select the proper value of \\nhyperparameters, feature selection, feature scaling, regularization, and kernel \\nselection.  \\n', metadata={'source': 'ml.pdf', 'page': 12}),\n",
       " Document(page_content=' \\n❖ XGBoost Regression:  \\n \\nXGBoost (eXtreme Gradient Boosting) is a powerful and popular machine -\\nlearning  algorithm known for its efficiency and high performance across a \\nwide range of tasks. It belongs to the ensemble learning category, which \\nmeans it combines the predictions of multiple base models (often decision \\ntrees) to improve overall performance.  Suppose we have this data, and we \\nwant to predict the drug effectiveness.  \\n                                      \\nThe first step in fitting XGBoost to training data is to make an initial prediction \\nwhich is basically the mean of the target variable in the training data set. In the \\nabove example, the initial prediction is 0.5. Then we calculate the residuals \\nwhich is t he difference between the observed and predicted value. So \\naccording to the above data set the residual values are : \\n                                         \\nNow we do calculations for similarity gain and the formula for this is,  \\n            \\n', metadata={'source': 'ml.pdf', 'page': 13}),\n",
       " Document(page_content='Where lambda is a hyperparameter and we are assuming it is 0 here. Now we \\ncalculate the similarity gain of the root that is 0. Now for splitting checks which \\nwill be better. So, for this, we will take an average of the first 2 data sets and \\nthat is the average whose value is 15. Now make dosage less than 15 a root \\nnode and we will calculate a gain first we will calculate the similarity score of \\neach node.  \\n                                    \\n                                    \\n                             \\nWe calculated the similarity score now we will calculate the gain.  \\n                              \\n', metadata={'source': 'ml.pdf', 'page': 14}),\n",
       " Document(page_content='So the gain is 120.33. We calculated the gain for a threshold dosage of less \\nthan 15. Now we will take the other 2 data sets and calculate an average that is \\n22.5 Now we will calculate the gain for 22.5.  \\n                                          \\nThe gain of this node is 4. Now we will take other values and compute the \\naverage for new threshold value that is dosage less than 30.  \\n              \\nNow we will select the threshold that will calculate the largest gain that is \\ndosage less than 15.  \\n                                           \\nNow as in left node there is only one residual we cannot split it further but we \\ncan split the right node. Now we will again do the same procedure as explained \\nabove. Now we will look into data for values greater than 15. The next node is \\nless than 22.5.  \\n', metadata={'source': 'ml.pdf', 'page': 15}),\n",
       " Document(page_content='                                             \\nWe calculate the similarity score of node.  \\n                                                                    \\nAnd gain for the node is below.  \\n                                 \\nNow we calculate the gain of next threshold. That is,  \\n        \\n', metadata={'source': 'ml.pdf', 'page': 16}),\n",
       " Document(page_content='As a dosage less than 30 has a high value of gain we will select this as a node.  \\n                                              \\nNow let’s discuss the pruning of a tree for that we use a hyperparameter called \\ngamma. Pruning is done based on the value of gain whether the subtree has to be \\nremoved or not. So if the difference between gain and gamma is negative we \\nremove the subtree we  do not remove it. Now if we consider the value of gamma \\nas 130 initially and do 140.17 – 130 answer will be positive so we will not remove \\nthe subtree. For the root node, the difference is negative but we will not remove it \\nbecause we haven’t removed the child nodes of the root node. Here is how \\npruning is done. We can also set the limit of the depth of a tree.  Now we will \\ncalculate the output value of each leaf node.  \\n                        let lambd a=0. \\n                                 \\n', metadata={'source': 'ml.pdf', 'page': 17}),\n",
       " Document(page_content='                                             \\n                               \\nNow we can use the above tree  to make new predictions. For that the formula is,  \\n  Predicted value initial prediction + (learning rate * output value)  \\n \\nSo for data 13 dosages, the predicted value is,  \\n                                            \\nFor a dosage equal to 20, the predicted value is 2.6. Now we will repeat the steps \\nwith all values.  \\n                                    \\n', metadata={'source': 'ml.pdf', 'page': 18}),\n",
       " Document(page_content='Now we will build another tree based on new residuals and make new predictions \\nthat give us even smaller residuals. V alues of predicted values  that we got from \\nthe first tree  then we used  these values and calculated their mean which is used \\nas the initial predictor for building the second tree and so on.  This algorithm \\npredicts highly accurate values. Training an XGBoost model with a large number of \\ntrees and deep trees can be resource -intensive in terms of memory and \\ncomputation.  It is applicable to a wide  range of data sets. If overfitting occurs then \\nwe do limit the number of trees, limit the depth of trees, etc.  \\n \\nClassification:  \\nClassification in supervised machine learning is like teaching a computer to \\nrecognize and sort things into different groups based on their unique \\ncharacteristics. It\\'s like how we classify objects in our daily lives.  \\nExample: Disease detection etc.  \\nAlgorithms:  \\n \\n❖ Logistic Regression:  \\nLogistic regression is a type of statistical model used for classification tasks \\nin machine learning. It\\'s particularly useful when the target variable (what \\nyou\\'re trying to predict) is categorical. This means it can have only two \\npossible outcomes, such as \"yes\" or \"no\", \"spam\" or \"not spam\", etc.  Unlike \\nlinear regression, where the output can be any real number, logistic \\nregression outputs probabilities. These probabilities are constrained to be \\nbetween 0 and 1.  Logistic regression uses the logistic function (also known \\nas the sigmoid function) to model the relationship between the features \\nand the probability of a specific outcome.  The decisio n boundary is a \\nthreshold value that separates the classes. If the predicted probability is \\ngreater than the threshold, it assigns the data point to one class, otherwise \\nto the other.  The most common loss function used in logistic regression is ', metadata={'source': 'ml.pdf', 'page': 19}),\n",
       " Document(page_content=\"the log -likelihood loss, which measures the difference between predicted \\nprobabilities and actual outcomes.  \\n \\n \\n                    \\n \\nThis model is well used for binary classification. So in order to avoid \\noverfitting in logistic regression we do regularization.  \\n \\n❖ KNN:  \\n \\nK-Nearest Neighbors (KNN) regression is a simple and intuitive machine \\nlearning algorithm used for classification  tasks. It's an instance -based \\nlearning method that makes predictions by considering the average (or \\nweighted average) of the target values of its k -nearest neighbors in the \\n\", metadata={'source': 'ml.pdf', 'page': 20}),\n",
       " Document(page_content='training data . During training, the KNN algorithm stores the entire training \\ndataset.  \\nTo make a prediction for a new data point, the algorithm identifies the k -\\nnearest data points (neighbors) in the training set based on a distance \\nmetric (usually Euclidean distance). K is a hyperparameter here. The \\ndistance is calculated between the entere d point from all points in the \\nwhole data set then the data set is sorted based on distance in ascending \\norder and picked the first k rows and the mode  of target variables are \\ncalculated which is the answer. The value of k is a hyperparameter that you \\nneed  to specify. A smaller k (e.g., 1 or 3) makes the model sensitive to noise \\nin the data and can result in a more variable prediction. A larger k (e.g., 10 \\nor 20) provides a smoother prediction but might not capture local patterns \\nas effectively. KNN classifier  is simple to understand and implement. It can \\ncapture complex and nonlinear relationships between features and the \\ntarget variable. Choosing the appropriate value of k is crucial and can be \\nchallenging. KNN can be computationally expensive when the dat aset is \\nlarge, as it requires calculating distances to all data points during prediction. \\nKNN classifier  works well for small to moderately -sized datasets with a \\nreasonable number of features. For large datasets, the computational cost \\nof finding nearest neighbors can become prohibitive. If overfitting occurs in \\nKNN regression then do better feature selectio n, adjust the value of k, do \\nfeature selection etc.  \\n \\n❖ Naïve Bayes Algorithm:  \\n \\n', metadata={'source': 'ml.pdf', 'page': 21}),\n",
       " Document(page_content='Naive Bayes is a simple and intuitive machine learning algorithm used for \\nclassification tasks. It\\'s based on Bayes\\' theorem and assumes that the \\nfeatures used to make predictions are independent of each other (which is \\nwhy it\\'s called \"naive\").  Naive Bayes relies on Bayes\\' theorem .  \\n \\nConsider above is a data set and we have to use naïve Bayes to predict \\nwhether we can play tennis or not. So we first calculate the prior \\nprobabilities based on the given data what is the probability that yes we can \\nplay tennis and no we cannot? After that , we calculate the conditional \\nprobabilities of all input features as shown below.  \\n                          \\n                 \\nThen using the above data we do calculations for the testing data. So we \\nhave to do a prediction for the following data.  \\n \\n', metadata={'source': 'ml.pdf', 'page': 22}),\n",
       " Document(page_content=' \\nAs the probability of no is higher than yes we will predict that tennis cannot \\nbe played. This is an example of multinomial naïve bayes.  \\n \\nNow in the above example, all input features were discrete what if the input \\nfeatures have continuous values then we cannot calculate the conditional \\nprobability We will use Gaussian Naïve Bayes.  \\n                      \\nWe have to predict using data that the person is male or female so we will \\ncalculate the mean and standard deviation of each input feature for male \\nand female and first we calculate the prior probabilities.  \\n                                     \\n', metadata={'source': 'ml.pdf', 'page': 23}),\n",
       " Document(page_content='                      \\nNow we will calculate for all  \\n \\nNow we have to predict whether  a person is male or female based on the \\nfollowing data  \\n \\nFor that, we can use the Gaussian distribution equation.  \\n                                           \\nUsing Gaussian distribution we calculate the following values  \\n \\n    As the posterior probability of female is more so we will predict that \\nperson will be female. Naive Bayes is computationally efficient . It can \\nhandle a large number of features, making it suitable for high -dimensional \\n', metadata={'source': 'ml.pdf', 'page': 24}),\n",
       " Document(page_content='data sets.  Sometimes we face an issue called zero probability issue and to \\ndeal with that we use Laplace smoothing.  \\n                                 \\nYou can see for overcast|No  the probability is zero.  \\n \\nNow probability will change from 0 to some positive value.  \\n         \\nIf overfitting occurs in naïve Bayes use feature selection, Laplace smoothing, \\netc. \\n❖ Decision Tree : \\n \\nImagine you have a dataset with different types of fruits, and you want to \\nclassify them as either apples, oranges, or bananas. A decision tree \\nalgorithm helps make these classifications based on features like color, size, \\nand texture.  The goal  of decision trees, is to partition the data into subsets \\nbased on the input features, leading to decisions or predictions.  There is a \\nroot node that is the first node of a decision tree also you can say it’s a node \\nthat does not have a parent. Then there is a leaf node which is the node \\nwho do not have child nodes. The other are decision nodes that are in \\nbetween these 2 nodes . Below is the data and its corresponding tree.  \\n', metadata={'source': 'ml.pdf', 'page': 25}),\n",
       " Document(page_content=' \\n \\nNow we must select the root node among the input features which one will \\nbe a root node. So first we will keep all inputs as root and will do calculation \\nlike if the root node is popcorn then if someone likes popcorn then will it \\nloves cool as ice or not o r if someone who does not love popcorn will \\nsomeone love cool as ice or not.  \\n                       \\n                           \\nYou have seen that 2 leaves of popcorn and one leaf of soda contain a \\nmixture of people who love as cool as ice, and some do not it is called \\n', metadata={'source': 'ml.pdf', 'page': 26}),\n",
       " Document(page_content='impure. While the right leaf of soda does not contain the mixture it is called \\npure.  There are several ways to quantify the impurity known as entropy, \\ninformation gain, and Gini impurity. We will calculate the Gini impurity of \\nnodes. We calculate it as below. First we calculate the Gini impurity of \\nleaves. For the left leaf:  \\n \\nFor right leaf:  \\n  \\nNow the total Gini impurity of a node is calculated as,  \\n \\nWe do the same calculations for soda. For input feature age as it  contains \\nthe continuous value, we first sort it in ascending order and calculate the \\naverage of 2 adjacent age values and then calculate the Gini impurity value \\nof each average age.  \\n', metadata={'source': 'ml.pdf', 'page': 27}),\n",
       " Document(page_content='                      \\nNow here is an example of how we calculate the Gini impurity value of each \\naverage values.  \\n \\n', metadata={'source': 'ml.pdf', 'page': 28}),\n",
       " Document(page_content=' \\n \\nIn the same way, we calculate the Gini impurity for all average values and \\nselect the minimum one. Now we see among all the gini impurity value of \\nsoda was minimal  all so we selected soda as the root node.  \\n                          \\nNow we see the left node is n=impure so will split it in order to reduce the \\nimpurity. We will follow the same steps as explained earlier and will select \\nthe node with the minimum value. As by calculations we observed age less \\nthan 12.5 has less gini impur ity so we will select it as a node. so in the \\ndecision  tree which depth tree would  be good with a depth of 5 or 7 , we \\nbuilt different trees  to check their  accuracy and selected  the one  with the \\n', metadata={'source': 'ml.pdf', 'page': 29}),\n",
       " Document(page_content='best accuracy and best performed  on the training set . Decision trees are \\neasy to interpret and visualize.  Can handle both categorical and numerical \\ndata.  Can capture complex relationships in data.  But they may be sensitive \\nto small variations in data and in order to avoid overfitting in the decision \\ntree we do pruning.  \\n❖ Random Forest Classifier:  \\n \\nRandom Forest is an ensemble learning method used for both classification \\nand regression tasks in machine learning.  It builds multiple decision trees \\nduring training and merges their outputs to get a more accurate and stable \\nprediction.  Each tree in a Random Forest is trained on a different random \\nsample of the training data, allowing it to learn different aspects of the \\ndata.  Randomly select a subset of data (with replacement) for training each \\ntree. Some data points may be included multiple times, while others may \\nnot be included at all  called out of the bag data set.  So using this data set \\nwe can check the accuracy of random forest.  The portion of out of ba d data \\nset that is incorrectly classified is called out of bad error. This is usually \\nknown as bootstrapping. The data  set is called a bootstrap data set. Then  \\nwe create the decision tree for each data set using above explained \\ninformation.  Random forest can handle large data and its accuracy is better \\nthan decision tree.  But it requires more computational resources.  In order \\nto avoid overfitting do pruning ( limit the depth of a tree), etc.  \\n \\n❖ Support Vector Machine:  \\n \\nSupport Vector Machines, often abbreviated as SVM, are a powerful class of \\nsupervised machine learning algorithms used for classification and \\nregression tasks. They are particularly effective when dealing with complex \\ndatasets where there is no clear linear separation between different classes \\nor groups.  The primary goal of SVM is to find the best possible boundary (or \\nhyperplane) that can effectively separate different classes in the dataset. \\nThis boundary is known as the \"decision boundary\" or \"hyperplane.\"  SVM \\naims to maximize the mar gin between the decision boundary and the \\nnearest data points from each class. These nearest data points are called ', metadata={'source': 'ml.pdf', 'page': 30}),\n",
       " Document(page_content='\"support vectors.\" Maximizing the margin not only ensures a good \\nseparation but also enhances the model\\'s generalization to unseen data.  In \\ncases where the data isn\\'t linearly separable in its original feature space, \\nSVM uses a \"kernel trick\" to map the data into a higher -dimensional space \\nwhere linear separation becomes possible. Common kernel functions \\ninclude polynomial kernels and radia l basis function (RBF) kernels.  Training \\nan SVM involves minimizing a cost function. This function tries to find the \\noptimal hyperplane that maximizes the margin while minimizing \\nclassification errors.  To classify a new data point, you apply the learned \\ndecision boundary. You check which side of the hyperplane it falls on.  Like in \\nan equation of decision boundary , the values are entered if its positive it\\'s \\nclassified into one class otherwise,  if negative then classified to another \\nclass.  The cost function is  \\n                                  \\nWhere c is a hyperparameter that tells how many data points can be \\nmisclassified. While the other parameter tells the distance between \\nmisclassified data points and marginal planes.  \\n \\n❖ XGBoost Classifier:  \\n \\n \\nXGBoost (eXtreme Gradient Boosting) is a powerful ensemble learning \\nalgorithm known for its efficiency and high performance across a wide \\nrange of tasks, including classification . XGBoost is an ensemble learning \\nmethod that combines the predictions of multiple weak learners (usually \\ndecision trees) to create a stronger, more accurate model. It does this by \\niteratively building trees and then combining their predictions.  We basically \\nuse the initial prediction for building an initial tree that is 0.5 means 50% \\nchanc e of happening. The data set is below,  \\n', metadata={'source': 'ml.pdf', 'page': 31}),\n",
       " Document(page_content='                        \\nNow we calculate the residual values by subtracting the training data set and the \\ninitial predicted value.  \\n                                                      \\nThe formula for the similarity score is,  \\n            \\nNow the similarity score for root is 0 which is of residual values. Now for building \\na tree, we first have to choose the root node that has a high value of gain. For this, \\nwe took the last 2 data points and calculated it average which is 15 which is the \\nthreshold value.  \\n                                    \\n \\n \\n', metadata={'source': 'ml.pdf', 'page': 32}),\n",
       " Document(page_content='                         \\nSo similarity score of a left node is 0.33.  \\n                                   \\nThe similarity score of the right node. Now calculate the gain the formula is  \\n                             That  is 1.33.  \\nNow check the same gain value for other threshold values  and calculate the gain.  \\nAnd by calculation dosage less than 15 has more gain so it will be root node.  \\n                                                    \\nNow as there is only one residual in right node so we will not split it but will split \\nthe left node. It has 3 residual values so select the thershold values and check \\nwhich thershold gives the high value of gain then select it. So dosage less than 5 \\nwill be selected.  \\n                                                             \\n', metadata={'source': 'ml.pdf', 'page': 33}),\n",
       " Document(page_content='The minimum number of residuals in each leaf is determined by calculating the \\ncover. When XGBoost is used for classification  the cover is equal to  \\n                 \\nFor regression the cover equals to  \\n                   \\nIn order to avoid overfitting we do pruning we prune by calculating the difference \\nbetween gain and gamma. If the answer is positive we do not remove the node \\nbut if the difference is negative we prune the node.  Now for classification the \\noutput value for leaves are :  \\n         \\n                    \\n', metadata={'source': 'ml.pdf', 'page': 34}),\n",
       " Document(page_content='                              \\nSo for making a prediction, we use the initial prediction value.  We need to \\nconvert the probability to log(odds) value.  \\n                                        \\n                                                                       \\nThe value of log(odds) is 0. So the predictions are made using,  \\n                      0.3 is \\nlearning rate.  \\nTo convert the log(odds) value into probability we plug it into the logistic function.  \\n', metadata={'source': 'ml.pdf', 'page': 35}),\n",
       " Document(page_content='                                                      \\n                                               \\n                                                       \\nThe residual value gets shorter now. Now we built the secind tree using new \\nresiduals and so on. For the second tree the residuals are  \\n                                                              \\nThe similarity score will look like,  \\n        \\n \\nIt is known for its high accuracy.  \\n \\n \\n \\n \\n \\n \\n \\n', metadata={'source': 'ml.pdf', 'page': 36}),\n",
       " Document(page_content='Un-Supervised Learning : \\nUnsupervised learning is a type of machine learning where the algorithm learns \\nfrom unlabeled data. Unlike supervised learning, there are no target labels \\nprovided. Instead, the algorithm identifies patterns, relationships, and structures \\nin the data on its own.  \\nSome examples of unsupervised learning are anomaly detection, pattern \\nrecognition, audience segmentation, etc.  \\n \\nTypes of supervised learning tasks:  \\n \\n1. Clustering  \\n2. association  \\nClustering : \\nUnsupervised clustering is a type of machine learning where the algorithm tries to \\nfind natural groupings or clusters within a dataset without being provided with \\nany specific labels or target information.  \\nExamples of clustering include image segmentation, anomaly detection, etc.  \\nAlgorithms:  \\n❖ K-means : \\n \\nK-means is a centroid -based clustering algorithm, where we calculate the \\ndistance between each data point and a centroid to assign it to a cluster. \\nThe goal is to identify the K number of groups in the dataset.  It is an \\niterative process of assigning each data point to the groups and slowly data \\npoints get clustered based on similar features.  Here, we divide a data space \\ninto K clusters and assign a mean value to each. The data points are placed \\nin the clusters closest to the mean value of that cluster.  Let consider the \\ndata we have g iven below,  ', metadata={'source': 'ml.pdf', 'page': 37}),\n",
       " Document(page_content='         \\n \\nThe first step is to define the K number of clusters in which we will group \\nthe data. Let’s select K=3.  The centroid  is the center of a cluster but initially, \\nthe exact center of data points will be unknown so, we select random data \\npoints and define them as centroids for each cluster. We will initialize 3 \\ncentroids in the dataset.  \\n                   \\n', metadata={'source': 'ml.pdf', 'page': 38}),\n",
       " Document(page_content='Now that centroids are initialized, the next step is to assign data points Xn \\nto their closest cluster centroid Ck  this is done by calculating the Euclidean \\ndistance between the data set and centroid and assigning a data point to a \\ncluster where the distance between data point and a particular cluster’s \\ncentroid is less.  \\n                   \\nNext, we will re -initialize the centroids by calculating the average of all data \\npoints of that cluster.  \\n                   \\n', metadata={'source': 'ml.pdf', 'page': 39}),\n",
       " Document(page_content='We will keep repeating steps 3 and 4 ( finding the distance between data \\npoints and centroids and after that reinitializing the centroids) until we have \\noptimal centroids and the assignments of data points to correct clusters are \\nnot changing anymore.  \\n                       \\nHere we clustered the data set. Now for initializing the value of k, there are \\nmany ways we can also select the k centroids randomly or we can use the \\nmost appropriate method k -means ++. K-means++ is a smart centroid \\ninitialization method for the K -mean algorithm . There are simple steps first \\nrandomly pick the first centroid then calculate the distance between all data \\npoints and the selected centroid , and select the one with the farthest \\ndistance from the first centroid . Repeat the steps until you get the k \\ncentroids.  \\nNow discuss how to know the value of k for that we use the famous elbow \\nmethod. We select a range of k values like 1,2,3,4 and so on. Then find the \\ndistance between data points and the centroid and find the squared sum \\ndistance. Create a plot where the x -axis represents the number of clusters \\n(K) and the y -axis represents the corresponding SSD values.  Repeat the \\nsteps for other values of k and then select the elbow point.  \\n', metadata={'source': 'ml.pdf', 'page': 40}),\n",
       " Document(page_content='             \\nThe distortion (or inertia) in k -means clustering measures how spread out \\nthe data points are within each cluster. It\\'s calculated as the sum of the \\nsquared distances between each data point and its corresponding cluster \\ncentroid.  As you increase the number of clusters (k), the distortion tends to \\ndecrease. This is because as you add more clusters, each data point tends to \\nbe closer to its nearest centroid. So, initially, adding more clusters leads to a \\nsignificant reduction in dist ortion.  However, after a certain point, adding \\nmore clusters doesn\\'t result in a significant reduction in distortion.  The \\n\"elbow point\" is where the distortion starts to flatten out. It\\'s the point \\nwhere the rate of decrease sharply changes, forming an \"elbow\" shape in \\nthe distortion vs. k plot.  \\nK-Means is easy to understand and implement, making it a quick and \\nefficient clustering method.  It can handle large datasets efficiently, making it \\nsuitable for big data applications.  K-Means is computationally faster \\ncompared to other clustering algorithms, making it suitable for real -time \\napplications.  But, You need to specify the number of clusters in advance, \\nwhich can be challenging in some cases.  Outliers can significantly impact \\nthe clustering results, potentially leading to inaccurate cluster assign ments.  \\nHere are some steps you can take to address overfitting in K -Means :  \\nReduce the number of clusters(k), address outliers , etc. \\n', metadata={'source': 'ml.pdf', 'page': 41}),\n",
       " Document(page_content='❖ Hierarchal  Clustering:  \\nHierarchical clustering is a method used to group together similar data \\npoints based on their features. It creates a tree -like diagram called a \\ndendrogram, which shows the arrangement of clusters.  \\nHierarchal clustering is of two types:  \\n• Agglomerative clustering  \\n• Divisive clustering  \\nIn Agglomerative Hierarchical Clustering each data or observation is treated \\nas its cluster. A pair of clusters are combined until all clusters are merged \\ninto one big cluster that contains all the data.  \\nIn Divisive Hierarchical Clustering , entire data or observation is assigned to a \\nsingle cluster. The cluster is further split until there is one cluster for each \\ndata or observation.  \\nSo, both are the reverse of each other here I will explain agglomerative \\nclustering.  \\nSo, in agglomerative clustering for each point initially , we will consider it as \\na separate cluster. Next, we will find the nearest point and create a new \\ncluster. So, we keep on repeating the steps until we get a single cluster.  \\n                                   \\n \\n \\nNow to check how many clusters are there we use a dendogram.  \\n', metadata={'source': 'ml.pdf', 'page': 42}),\n",
       " Document(page_content='                                    \\nLike the first clusters of P1 and P2 are formed the P4 and P5 and we keep \\ncontinuing steps to check for the nearest points and form clusters now the \\nthing is what will be the value of k ? For we take a threshold value and for a \\nthreshold , we take the longest vertical line where no horizontal line passes  \\nthrough it. Like threshold may be of distance 5,3 or 1.  \\n                              \\nSo the threshold found at a distance equals 5 and will draw a horizontal line \\nand will check how many points line has cut in the above case 2 points are \\nformed by cutting the vertical line so value of k will be 2. Here is how we \\nchoose threshold value.  \\nSo, divisive clustering starts with all data points in a single cluster and then \\nrecursively divides them into smaller clusters.  In divisive clustering, we start \\nwith all data points belonging to one large cluster.  We look for the cluster \\nthat is the least cohesive (i.e., it contains data points that are less similar to \\neach other). This cluster is then split into two smaller clusters.  This process \\n', metadata={'source': 'ml.pdf', 'page': 43}),\n",
       " Document(page_content=\"of identifying the least cohesive cluster and splitting it continues recursively \\nuntil each data point is in its own cluster.  Similar to agglomerative \\nclustering, you can create a dendrogram to visualize the process.  To decide \\non the number of clusters, you can use a similar approach as in \\nagglomerative clustering. Look for the threshold where you get the desired \\nnumber of clusters.   \\nThe algorithm is used well if the data set is small if we have a large data set \\nthen k means will be good.  Hierarchical clustering is less sensitive to outliers \\ncompared to K -means.  Hierarchical clustering can be more computationally \\ndemanding, especially for large datasets  \\n \\nAnomaly Detection:  \\nAnomaly detection, also known as outlier detection, is the process of identifying \\ndata points or patterns that deviate significantly from the norm in a dataset. These \\ndeviations are often indicative of unusual or unexpected behavior, which may \\nwarrant furt her investigation.  Anomaly detection is widely used across various \\ndomains, including cybersecurity, fraud detection, healthcare, manufacturing, \\nfinance, and more. It helps identify rare events or irregularities that may have \\nsignificant implications.  In a Gaussian (Normal) Distribution, the probability \\ndensity is highest at the mean and decreases symmetrically as you move away \\nfrom the mean in both directions. This is what creates the characteristic bell -\\nshaped curve.  \\nAlgorithm:  \\n• Choose n features xi that you think might be indicative of anomalous \\nexamples  (Create  a histogram for the feature. This shows the distribution of \\nvalues. In a Gaussian Distribution, you'll see a bell -shaped curve.  Does it \\nresemble a bell curve? If so, it's an indicator that the data might follow a \\nGaussian Distribution.  If the curve is heavily skewed, the data might not \\nfollow a Gaussian Distribution.  So, you can apply mathematical \\ntransformations to the data to make it more Gaussian -like.) \", metadata={'source': 'ml.pdf', 'page': 44}),\n",
       " Document(page_content=\"     \\n \\n• Find the mean and variance of each input feature.  \\n \\n• Given new example x compute p(x),  \\n          \\n• Anomaly if p(x)<epsilon ( a threshold value)  \\nNow anomaly detection can be used over supervised learning in many cases like, \\nWhen you have limited or no labeled data for anomalies.  When you don't have a \\nclear understanding of all possible types of anomalies. Anomaly detection can \\ndiscover unknown anomalies without prior knowledge.  \\n \\n \\nRecommendation Systems : \\nRecommendation systems are intelligent algorithms that assist users in \\ndiscovering relevant and personalized content, products, or services. They play a \\nvital role in enhancing user experiences across various platforms, from e -\\ncommerce websites to streaming services.  Recommendation systems have \\nrevolutionized the way users interact with platforms and services. They not only \\nimprove user engagement but also drive sales, increase customer satisfaction, and \\nfoster loyalty.   \\nExample: Amazon items recommendation, Netflix movie recommendation.  \\n\", metadata={'source': 'ml.pdf', 'page': 45}),\n",
       " Document(page_content=\"There are 2 common techniques used in recommendation systems explained \\nbelow.  \\nTechniques : \\n❖ Collaborative Filtering:  \\n \\n“Linear regression is used to predict the ratings that a user might give to a \\nmovie they haven't seen yet. The idea is to find a linear relationship \\nbetween the features (such as user behavior, movie characteristics, etc.) and \\nthe ratings. This allows us to  estimate what rating a user might give to a \\nmovie based on their historical preferences and characteristics of the \\nmovie. ” \\n \\n“Logistic regression, on the other hand, is used in a different part of the \\nrecommendation system. It can be used to answer binary questions like Did \\nuser j watch the movie I after being shown? This involves a yes/no \\nprediction, which is a classification problem.  Logistic regression is well -\\nsuited for this because it models the probability of a binary outcome.  In a \\nrecommendation system, this could be used to predict whether a user will \\nwatch a recommended movie or not based on their behavior and \\npreference s.” \\n \\nFor working on movie recommendations,  the model is the same as the \\nlinear regression:  \\n \\n \\n \\nWe have movie and user matrix data and entries are rated  and there are \\nmany missing values so our goal is to build  a model that predicts it well.  \\n \\n\", metadata={'source': 'ml.pdf', 'page': 46}),\n",
       " Document(page_content='       \\n \\nso, for user and item parameters are trained and cost function is reduced \\nusing gradient descent , and the correct value of parameters is obtained . \\nFirst of all , we will calculate the parameters of movies and after that , we will \\ncalculate for parameters of users  to predict the ratings for missing values for \\nchoosing the number of parameters of movies we use cross -validation  to \\nuse different values and then select which gives high accuracy.   \\nThe cost to learn parameters for user j is,  \\n \\n \\nTo learn parameters for all users the overall cost function is below,  \\n \\nNu=> number of users  \\n \\nNow to learn the parameters for the movie the cost function is,  \\n \\n \\n \\n \\n', metadata={'source': 'ml.pdf', 'page': 47}),\n",
       " Document(page_content=' \\nTo learn the parameters for all movies the overall cost function is,  \\n \\n \\n \\nnm=> total number of movies.  \\n \\nNow, the overall cost function to learn the parameters for users and movies \\nis given below,  \\n \\n \\n \\nTo reduce the cost function and get the best parameters we may use \\ngradient descent.  \\n \\n \\nNow using the model we fill in the missing values so we recommend the \\nmovies user j has liked the sci-fiction  movies so we recommend other sci-\\nfiction  movies highly rated by other people to user j predicting  that user j \\n', metadata={'source': 'ml.pdf', 'page': 48}),\n",
       " Document(page_content=\"may like this . In collaborative filtering, you don't have to explicitly choose \\nwhether to find the best parameters for users or movies first. The process \\ninvolves updating both sets of parameters (users and movies) in an iterative \\nmanner.  \\n \\nHere's a simplified step -by-step process:  \\n \\nStart with initial parameter values for both users and movies.  \\nUpdate movie parameters while keeping user parameters fixed.  \\nThen, update user parameters while keeping movie parameters fixed.  \\nRepeat steps 2 and 3 in an iterative manner.  \\nContinue this process until the model converges (meaning the cost function \\nstabilizes or changes very slowly).  \\nThis iterative process allows the model to gradually learn the best \\nparameters for both users and movies simultaneously. So, there's no \\nspecific order to follow; you update both sets of parameters in a back -and-\\nforth manner until the model learns the relati onships between users and \\nmovies.  \\n \\nNow for binary classification cases used in recommendation system, the \\nequations are below,  \\n \\n \\n \\n\", metadata={'source': 'ml.pdf', 'page': 49}),\n",
       " Document(page_content=' \\n \\n \\n \\n❖ Content -based  Filtering:  \\n \\n \\n \\nContent -based filtering is another important technique used in \\nrecommendation systems. Unlike collaborative filtering, which relies on \\nuser -item interactions and similarities, content -based filtering focuses on \\nthe attributes or features of items and users . \\nFor example if we are working on a movie recommendation system,  \\n \\n', metadata={'source': 'ml.pdf', 'page': 50}),\n",
       " Document(page_content=' \\n \\n \\n \\nFor finding these vectors we will use neural network.  \\n \\n', metadata={'source': 'ml.pdf', 'page': 51}),\n",
       " Document(page_content=\" \\n \\nThe cost function will be,  \\n \\n \\n \\nIf the dot product between a user's preference vector and the feature \\nvector of a sci -fi movie is large, it indicates a high level of alignment or \\nsimilarity. This suggests that the user's preferences are well -matched with \\nthe features associated with sci -fi movies.  \\nIf the distance above is small then it means movies are similar if we find the \\ndistance between the vectors of Interstellar  and The Martian  the distance \\nwill be small.  So if a user likes science fiction movies such movies will be \\nrecommended to users which are more similar to science fiction.  \\n \\n \\nSummary:  \\n \\nBoth collaborative filtering and content -based filtering have their strengths \\nand weaknesses. The choice between them depends on the specific context \\nand requirements of the recommendation system.  \\n\", metadata={'source': 'ml.pdf', 'page': 52}),\n",
       " Document(page_content=\" \\nCollaborative filtering doesn't require explicit feature extraction. It learns \\npreferences directly from user -item interactions, making it suitable for a \\nwide range of domains.  It requires a sufficient amount of  user -item \\ninteraction data to make accurate recommendations, which can be an issue \\nin sparse datasets.  Content -based filtering offers clear interpretability, as \\nrecommendations are based on explicit item features (e.g., genre, director).  \\nIt requires explicit feature extraction and engineering, which can be time -\\nconsuming and may require domain knowledge.  \\nIf you have a large dataset with significant user -item interactions, \\ncollaborative filtering can be very effective.  When you have detailed and \\nexplicit features for items that can be used for recommendation (e.g., \\ngenre, tags, attributes)  content -based filtering will be best.  \\nCombining collaborative and content -based filtering can often yield the best \\nof both worlds. Hybrid models can mitigate the limitations of individual \\nmethods.  \\n \\nDimensionality Reduction:  \\nTechniques : \\n❖ PCA ( Principal  Component Analysis):  \\n \\n“Process of figuring out the most important features that has the most \\nimpact on the target variable.”  \\n \\nPrincipal Component Analysis (PCA) is a technique used in data analysis and \\ndimensionality reduction. It helps find the underlying patterns in complex \\ndata by transforming it into a new coordinate system where the data's \\nvariability is maximized along the new axes, called principal components. \\nThis makes it easier to visualize and analyze the data while retaining as \\nmuch relevant information as possible . \\nWhen you have a high -dimensional dataset with many features (like 10), \\nvisualizing it directly becomes challenging because our visual perception is \", metadata={'source': 'ml.pdf', 'page': 53}),\n",
       " Document(page_content='limited to three dimensions at most.  PCA helps by transforming the data \\ninto a lower -dimensional space (like 2D or 3D) while retaining as much of \\nthe important information as possible. This makes it much easier to \\nvisualize and understand the relationships between data points.  \\n \\n \\n \\n \\nBefore applying PCA first you should do feature scaling.  \\n', metadata={'source': 'ml.pdf', 'page': 54}),\n",
       " Document(page_content=' \\n \\n \\n \\nConsider the data ,  \\n \\n', metadata={'source': 'ml.pdf', 'page': 55}),\n",
       " Document(page_content='                          \\n \\n \\nBest choice,  \\n', metadata={'source': 'ml.pdf', 'page': 56}),\n",
       " Document(page_content=' \\n \\n \\nNow lets understand the PCA.  \\nThe data set is below,  \\n                    \\nVisualization of data.  \\n \\n', metadata={'source': 'ml.pdf', 'page': 57}),\n",
       " Document(page_content='             \\n \\nFirst  we plot the data then we calculate the average measurement of Gen1 \\nthen for Gen 2.  \\n                 \\nNow we will shift the data so that the center is on the top of origin.  \\n', metadata={'source': 'ml.pdf', 'page': 58}),\n",
       " Document(page_content=\" \\nNow we will try to fit the line to it. We will draw ing  a random line that goes \\nthrough the origin then we rotate the line until it fits the data well.  \\n \\n \\nNow the question  is how PCA knows which line is fit or not let's look below.  \\nSo, first , we project the data onto the line then  for the best -fit line, either we \\nminimize the distance between  the data points and the line  or we try to find the \\nline that maximizes the distances from projected points to the origin.  \\nNow lets look into what PCA does to a single point.  \\n\", metadata={'source': 'ml.pdf', 'page': 59}),\n",
       " Document(page_content='                  \\nSo you can see that the distance between the point to the origin remains the \\nsame whether the line moves or not.  \\n                         \\n', metadata={'source': 'ml.pdf', 'page': 60}),\n",
       " Document(page_content='Now if the line fits best in the data then the b length will decrease and the c \\nlength will increase.        \\n \\nSo PCA fits the best line by maximizing the sum of squared distances from \\nprojected points to the origin. This line is called the principal component . \\nThis principal component is a linear combination of all input features.  \\nNow consider if you have 100 features and you cannot visualize it as it’s 100D. So \\nyou will reduce it to 2D so that you can visualize the data easily. Then you will \\ndraw a line called PC1 as explained above.  The goal is to find a line that captures \\nthe most variation in the data  means the majority of data points lie near the PC1.  \\nThen you will plot PC2 exactly perpendicular to PC1 without further optimization.  \\nNow, you have two lines (PC1 and PC2) that represent the most important \\ndirections of variation i n your data. Together, they form a new coordinate system.  \\nInstead of working with the original 100 parameters, you can now use these two \\ncomponents to represent your data points in a lower -dimensional space.  \\nFor example, if the equations for PC1 and PC2 are:  \\nPC1:   0.7 x1 + 0.3 x2  \\nPC2:   -0.3 x1 + 0.7 x2  \\nThen, for a given data point with original feature values  x1=3 and x2=4  you can \\ncalculate the corresponding coordinates  along PC1 and PC2.  \\nX* = 2.9  \\nY*= 2.5  \\nSo, x* and y* will be the new coordinates of the data point. You can then plot \\nthese transformed points to visualize the data in terms of the principal \\n', metadata={'source': 'ml.pdf', 'page': 61}),\n",
       " Document(page_content='components.  PCA assumes linear relationships and may not always capture \\ncomplex, non -linear patterns in the data.  ', metadata={'source': 'ml.pdf', 'page': 62})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Machine Learning  \\n \\nMachine learning is a subset of artificial intelligence (AI) that focuses on \\ndeveloping computer systems capable of learning and improving from data \\nwithout being explicitly programmed.  Machine learning is teaching computers to \\nlearn from data and make decisions or predictions based on that learning.  \\nMachine learning is widely used in various fields and applications  such as:  \\n• Face Recognition  \\n• Object Detection  \\n• Chatbots  \\n• Recommendation Systems  \\n• Autonomous Vehicles  \\n• Disease Diagnosis  \\n• Fraud detection  \\nAnd many more it is nowadays being used in almost all sectors including \\nhealthcare, education, business, construction, astronomy, etc.  \\n \\nTypes of Machine Learning : \\n \\nThere are many types of machine learning but the most famous types are:  \\n➢ Supervised learning  \\n➢ Unsupervised learning  \\n➢ Reinforcement learning', metadata={'source': 'ml.pdf', 'page': 0}),\n",
       " Document(page_content=\"Supervised Learning  \\n \\nIn supervised learning the algorithm is trained on labeled data which means \\ntraining data includes the input and output pairs. The goal is to learn the mapping \\nfrom input to output.  \\nX -> Y \\nSome examples of supervised learning include image classification, email spam \\ndetection, and predicting software.  \\n \\nTypes of supervised learning tasks:  \\n \\n1. Regression  \\n2. Classification  \\n \\nRegression:  \\nRegression is a type of supervised learning task where the algorithm's goal is to \\npredict a continuous numerical output or target variable.  In regression, the output \\nis a real -valued number, and the algorithm's objective is to learn a mapping from \\ninput features to this continuous output.  \\nExamples of regression tasks include home price prediction, black hole mass \\nprediction, stock price prediction, age estimation, etc.  \\nAlgorithms:  \\n❖ Linear Regression:  \\n \\nLinear Regression is a fundamental statistical and machine -learning\", metadata={'source': 'ml.pdf', 'page': 1}),\n",
       " Document(page_content='technique  for solving regression problems  used for modeling the \\nrelationship between a dependent variable (or target) and one or more', metadata={'source': 'ml.pdf', 'page': 1}),\n",
       " Document(page_content='independent variables (or features). It assumes that this relationship is \\napproximately linear, meaning that changes in the independent variables \\nhave a linear effect on the dependent variable.  \\n➢ Simple Linear Regression:  \\nIn simple linear regression, there is only one input feature.  \\nY= wx+b  \\nY= target variable  \\nW=slope of straight line  \\n X=input features  \\n b= y-intercept  \\n➢ Multiple Linear Regression:  \\nIn multiple linear regression, there is more than one input feature.  \\nY=w1x1+w2x2+w3x3+…. +b', metadata={'source': 'ml.pdf', 'page': 2}),\n",
       " Document(page_content='Note: There are 2 common issues in machine learning and statistical \\nmodeling that are underfitting  and overfitting . \\nUnderfitting:  \\nUnderfitting occurs when a model is too simple to capture the underlying patterns \\nin the data, both in the training set and unseen data.  It essentially means the \\nmodel is not complex enough to represent the relationships between the input \\nfeatures and the target variable.  This means it is not predicting the output \\ncorrectly and the accuracy of the model is very low.  Solutions to overcome this \\nproblem are:  \\n1) Increase model complexity.  \\n2) Add more features.  \\n3) Feature engineering (It is basically creating the new input feature using the \\nexisting input features For example for home price prediction you are given \\nthe width and length of a home you can simply create another feature \\nnamed area of home by multiplying  the width and length of home you \\nalready know).  \\n4) Remove noise (clean data to remove outliers).', metadata={'source': 'ml.pdf', 'page': 3})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##2) transform\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=20)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3) embedding\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##4) vector store\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(documents,OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'independent variables (or features). It assumes that this relationship is \\napproximately linear, meaning that changes in the independent variables \\nhave a linear effect on the dependent variable.  \\n➢ Simple Linear Regression:  \\nIn simple linear regression, there is only one input feature.  \\nY= wx+b  \\nY= target variable  \\nW=slope of straight line  \\n X=input features  \\n b= y-intercept  \\n➢ Multiple Linear Regression:  \\nIn multiple linear regression, there is more than one input feature.  \\nY=w1x1+w2x2+w3x3+…. +b'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##5) method1 - retrive via similarity search\n",
    "result=db.similarity_search('what is linear regression')\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is linear regression',\n",
       " 'context': [Document(page_content='independent variables (or features). It assumes that this relationship is \\napproximately linear, meaning that changes in the independent variables \\nhave a linear effect on the dependent variable.  \\n➢ Simple Linear Regression:  \\nIn simple linear regression, there is only one input feature.  \\nY= wx+b  \\nY= target variable  \\nW=slope of straight line  \\n X=input features  \\n b= y-intercept  \\n➢ Multiple Linear Regression:  \\nIn multiple linear regression, there is more than one input feature.  \\nY=w1x1+w2x2+w3x3+…. +b', metadata={'page': 2, 'source': 'ml.pdf'}),\n",
       "  Document(page_content=\"There are 2 common techniques used in recommendation systems explained \\nbelow.  \\nTechniques : \\n❖ Collaborative Filtering:  \\n \\n“Linear regression is used to predict the ratings that a user might give to a \\nmovie they haven't seen yet. The idea is to find a linear relationship \\nbetween the features (such as user behavior, movie characteristics, etc.) and \\nthe ratings. This allows us to  estimate what rating a user might give to a \\nmovie based on their historical preferences and characteristics of the \\nmovie. ” \\n \\n“Logistic regression, on the other hand, is used in a different part of the \\nrecommendation system. It can be used to answer binary questions like Did \\nuser j watch the movie I after being shown? This involves a yes/no \\nprediction, which is a classification problem.  Logistic regression is well -\\nsuited for this because it models the probability of a binary outcome.  In a \\nrecommendation system, this could be used to predict whether a user will\", metadata={'page': 46, 'source': 'ml.pdf'}),\n",
       "  Document(page_content='their larger values.  It can lead to faster convergence of gradient -based \\noptimization algorithms (like gradient descent).  \\n❖ Polynomial Regression:  \\n \\nPolynomial regression is a type of regression analysis used in machine \\nlearning and statistics to model relationships between variables when the \\ntraditional linear regression model is insufficient. It extends the concept of \\nlinear regression by allowing fo r more complex, nonlinear relationships \\nbetween the independent and dependent variables.  \\ny = b₀ + b₁x + b₂x² + ... + b ₙxⁿ \\n \\n \\n \\nThe degree of the polynomial (n) is a hyperparameter that you can choose \\nbased on the complexity of the relationship you want to capture.  \\nA higher degree allows the model to fit the data more closely but may also \\nlead to overfitting if not chosen carefully.  \\nIn order if overfitting occurs then reduce the degree of polynomial, collect \\nmore data do regularization.  \\n \\n❖ KNN regression:', metadata={'page': 6, 'source': 'ml.pdf'}),\n",
       "  Document(page_content='technique  for solving regression problems  used for modeling the \\nrelationship between a dependent variable (or target) and one or more', metadata={'page': 1, 'source': 'ml.pdf'})],\n",
       " 'answer': 'Linear regression is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. In simple linear regression, there is only one input feature, while in multiple linear regression, there are multiple input features. The goal of linear regression is to find a linear relationship between the independent variables and the dependent variable, where changes in the independent variables have a linear effect on the dependent variable. The equation for simple linear regression is Y = wx + b, where Y is the target variable, w is the slope of the straight line, x is the input feature, and b is the y-intercept. Multiple linear regression extends this concept to include multiple input features. Linear regression is commonly used in recommendation systems to predict ratings or make binary predictions based on historical data and characteristics of the items being recommended.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------\n",
    "##6) method2- retrive via retriver\n",
    "\n",
    "# 1)\n",
    "# from langchain_community.llms import Ollama\n",
    "# llm=Ollama(model=\"llama2\")\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI()\n",
    "# 2)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")\n",
    "# 3)\n",
    "## Create Stuff Docment Chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "\n",
    "# 4)\n",
    "retriever=db.as_retriever()\n",
    "\n",
    "# 5)\n",
    "## create_retrieval_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "\n",
    "response=retrieval_chain.invoke({\"input\":\"what is linear regression\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
